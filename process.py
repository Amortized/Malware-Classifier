import sys;
import glob;
from multiprocessing import Pool;
import time;
import gzip;
import numpy as np;
from sklearn.cross_validation import train_test_split;
from sklearn import ensemble;
from sklearn.multiclass import OneVsRestClassifier;
from sklearn.preprocessing import label_binarize;
import math;
import warnings


warnings.filterwarnings("ignore")

def readLabels(myfile):
  '''
    Generates a dict of labels
  '''
  labels = {};
  try :
    with open(myfile, "r") as f:
      next(f);
      for line in f:
        t = line.strip().split(',');
        labels[t[0].replace("\"", "")] = t[1];
  except :
    print("No files found ./data/");
    exit(1);
    
  return labels;
   
def genFeatures(myfile):
  '''
     Get the features
  '''
  try :
    lines = [line.strip().split()[1:] for line in open(myfile)]
  except :
    print("No files found ./data/");
    exit(1);

  features  = [0] * (16 ** 2 + 1);
  for line in lines:
    for record in line:
      #Count the byte occurence
      if record == '??':
        features[256] += 1;
      else:
        try :
          features[int(record,16)] += 1;
        except :
          print(myfile)
 
  return features;

def process(myfile, label=None):
  '''
     Generates signature and writes to a file
  '''

  features = genFeatures(myfile);


  with open(myfile+".features", "w") as f:
    for k in range(0, len(features)):
      if k < len(features) - 1:
        f.write(str(features[k]) + ",");
      else:
        f.write(str(features[k])); 

    if label:
      f.write("," + str(label));

    f.write("\n");
    f.close();
     

def process_wrapper(args):
  process(*args);

def preprocessing(input, training=True):
   """
      Computes the features  
   """
   bytes_files       =  glob.glob(input);

   already_processed =  glob.glob(input + ".features");
   already_processed =  [k.replace(".bytes.features","").replace("./data/test/","") for k in already_processed];

   bytes_files       =  [k for k in bytes_files if k.replace(".bytes","").replace("./data/test/","") not in already_processed];

   print(len(bytes_files))

   label_file  =  "./data/trainLabels.csv";

   start = time.time();

   labels  = readLabels(label_file);
   pool    = Pool();

   try :
     if training:
       params  = [(b_f, labels[b_f.replace(".bytes","").replace("./data/train/","")]) for b_f in bytes_files];
     else:
       params  = [(b_f, None) for b_f in bytes_files]; 
   except :
     pass

   results = pool.map( process_wrapper, params );

   pool.close();
   pool.join();

   print('total runtime of preprocessing = {0}'.format(time.time() - start));
 
def readFeatures(input, training=True):
  feature_files =  glob.glob(input);

  X = [];
  y = [];

  for myfile in feature_files:
    with open(myfile, "r") as f:
      for line in f:
        t = [int(val) for val in line.strip().split(',')];

        if training:
          X.append(t[:-1]);
          y.append(t[-1]);
        else:
          X.append(t);
          y.append(myfile.replace(".bytes.features","").replace("./data/test/",""));

  return np.array(X), np.array(y);

def buildModel_validate(X, y, params, validate_size):
  # shuffle and split training and test sets
  X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=validate_size, random_state=0);

  #Binarize the output
  y_train_b = label_binarize(y_train, classes=[1, 2, 3, 4, 5, 6, 7, 8, 9]);
  n_classes = y_train_b.shape[1];


  # Learn to predict each class against the other
  classifier = OneVsRestClassifier(ensemble.GradientBoostingClassifier(**params), -2);
  classifier.fit(X_train, y_train_b);

  return calculate_logLoss(classifier, X_train, y_train, n_classes), \
         calculate_logLoss(classifier, X_validate, y_validate, n_classes), \
         classifier;



def calculate_logLoss(classifier, X_validate, y_validate, n_classes):
  
  
  log_loss = 0;
  if len(X_validate) == 0:
    return log_loss;

  for i in range(0, len(X_validate)):
    #Predict for this data point
    y_hat  = classifier.predict_proba(X_validate[i])[0];

    #Normalize 
    y_hat  = y_hat / sum(y_hat);
    p_ij   = y_hat[y_validate[i]-1]
    p_ij   = max(math.pow(10, -15), p_ij);
    p_ij   = min(p_ij, 1 - math.pow(10, -15));
    

    log_loss += math.log(p_ij);

  log_loss /= len(X_validate);
  log_loss *= -1.0;

  return log_loss;

def gridSearch(X, y):
  loss    = dict();

  max_depth        = [3,4,5,6]; 
  min_samples_leaf = [1, 3, 5, 10]; 

  #Model param
  params = {'n_estimators': 1000, 'max_depth': 3, 'subsample': 0.9, 'max_features' : "sqrt", \
            'learning_rate': 0.01, 'min_samples_leaf': 5, 'random_state': 3};

  best_params       = params;
  min_validate_loss = sys.maxint;          

  count  = 0;
  for i in range(0, len(max_depth)):
    for l in range(0, len(min_samples_leaf)):
      #Set the parameters
      md  = max_depth[i];
      msl = min_samples_leaf[l];

      params['max_depth']                   = md;
      params['min_samples_leaf']            = msl;

      train_loss, validate_loss, classifier = buildModel_validate(X, y, params, 0.1); 

      loss[(md, msl)]                       = (train_loss, validate_loss);

      if validate_loss < min_validate_loss:
        min_validate_loss = validate_loss;
        best_params       = params;

      count                       += 1;

      print("No of Completed " + str(count));
      print(loss);

  print(loss);

  return best_params;

def predict_write(classifier, X_test, file_name):
  fh = open("./data/sampleSubmission.csv", "w");
  for i in range(0, len(X_test)):
    #Predict for this data point
    y_hat  = classifier.predict_proba(X_test[i])[0];

    #Normalize 
    y_hat  = y_hat / sum(y_hat);

    fh.write("\"" + str(file_name[i]) + "\",");
    for pr in range(0, len(y_hat)):
      if pr < len(y_hat) - 1:
        fh.write(str(y_hat[pr]) + ",");
      else:
        fh.write(str(y_hat[pr]));

    fh.write("\n");



if __name__ == '__main__':
  print("preprocessing Training data");
  #preprocessing("./data/train/*.bytes", True);
  print("preprocessing Test data");
  preprocessing("./data/test/*.bytes", False);

  
  print("Loading Training data");
  X_train, y_train    = readFeatures("./data/train/*.bytes.features", True);

  print("Loading Test data");
  X_test, file_name   = readFeatures("./data/test/*.bytes.features", False);


  print("Building a Model");
  #best_params      = gridSearch(X_train, y_train);
  best_params       = {'n_estimators': 1000, 'max_depth': 5, 'subsample': 0.9, 'max_features' : "sqrt", \
                       'learning_rate': 0.01, 'min_samples_leaf': 4, 'random_state': 3};

  #Build the best model
  train_loss, validate_loss, classifier = buildModel_validate(X_train, y_train, best_params, 0.1); 

  print("Predicting")
  predict_write(classifier, X_test, file_name);

  



